# -*- coding: utf-8 -*-
"""ResearchProject(Batch Normalization).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TDGA27GW14800jGkieusummnR1AucV09
"""



"""# **Building A Movie Recommandation System With Pytorch(Batch Normalization)**
In this project, we are trying to predict the ratings that a user will give to an unseen movie, based on the ratings he gave to other movies. We will use the movielens dataset.The Main folder, which is ml-100k contains informations about 100 000 movies. We will use AutoEncoders to create our recommandation system. Let's start by importing the required libraries.
"""

# AutoEncoders

# Importing the libraries

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
from torchvision import datasets, transforms
from torch.utils.data import Dataset
import time

import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()

"""# Batch Size 200


*   TRAINING LOSS: 0.0714
*   TEST LOSS: 0.0796
*   RMSE LOSS:0.2821
"""

training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

training_set.shape

test_set.shape

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

batch_size  = 200

''' Dataset Class'''
class DatasetR(Dataset):
    """Youtube-VOS dataset."""

    def __init__(self, training_set, nb_users, transform=None):
        super(DatasetR, self).__init__()
        """
        Args:
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.training_set = training_set
        self.nb_users = nb_users
    def __len__(self):
        return self.nb_users

    def __getitem__(self, idx):
        #print(idx)
        sample = self.training_set[idx]


        #sample = torch.Tensor(sample)


        return sample

class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Batch Normalization.
        self.bn1 = nn.BatchNorm1d(20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Batch Normalization.
        self.bn2 = nn.BatchNorm1d(10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Batch Normalization
        self.bn3 = nn.BatchNorm1d(20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)
        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        # x = self.do1(self.bn1(self.activation(self.fc1(x))))
        # x = self.do2(self.bn2(self.activation(self.fc2(x))))
        # x = self.do3(self.bn3(self.activation(self.fc3(x))))

        x = self.bn1(self.activation(self.fc1(x)))
        x = self.bn2(self.activation(self.fc2(x)))
        x = self.bn3(self.activation(self.fc3(x)))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

          
    def predict(self, x): # x: visible nodes
        x = self.forward(x)
        return x

#Creating an instance of our SAE class
sae = SAE()

dataset = DatasetR(training_set = training_set, nb_users = nb_users)
train_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)


datasetTest = DatasetR(training_set = test_set, nb_users = nb_users)
test_loader = torch.utils.data.DataLoader(datasetTest, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)




 #Defining a criterion which specifies the metric to minimize. In this case, we want to minimize the MSE (Mean Squared Error)
criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. In this case, we'll use RMSprop
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)

# Setting the number of epochs

losses = []

time_start = time.time()

nb_epoch = 200

# Iterating over each epoch

for epoch in range(1, nb_epoch + 1):

    #sae.train()

    # Initializing the train_loss which will be updated

    train_loss = 0

    # Initializing a counter

    s = 0.

    # Iterating over each user

    #for id_user in range(nb_users):

    for batch_idx, (sample) in enumerate(train_loader):

        # The input corresponds to the ratings given by the current user for each movie

        input = Variable(sample)

        target = input.clone()

        # We don't consider movies NOT rated by the current user. So we specify a conditional statement

        if torch.sum(target.data > 0) > 0:

            # We use our SAE to get the output from the 

            #print('input:  '+ str(input.shape))

            output = sae(input)

            #print(output.shape)

            target.require_grad = False

            output[target == 0] = 0

            # Defining our loss function, comparing the output with the target

            loss = criterion(output, target)

            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)

            # Computing the gradients necessary to adjust the weights

            loss.backward()

            # Updating the train_loss

            train_loss += np.sqrt(loss.data*mean_corrector)

            s += 1.

            # Updating the weights of the neural network

            optimizer.step()

    epoch_loss = train_loss / len(train_loader)
    losses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, losses, 'g', label='batch size = 200 ') # 'g' = color green
plt.title('Batch Normalization- epoch = 200')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')

plt.legend()
plt.show()

with torch.no_grad():
  sae.eval()
  test_loss = 0
  s = 0.
  for input1,target1 in zip(train_loader,test_loader):
      input = Variable(input1)
      target = Variable(target1)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
  print('test_loss: '+str(test_loss/s))



from math import sqrt

RMSE = sqrt(0.0796)

RMSE

"""# Batch size 500

*   TRAINING LOSS: 0.0583
*   TEST LOSS: 0.0619
*   RMSE LOSS:0.2487
"""



training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

training_set.shape

test_set.shape

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

batch_size  = 500

''' Dataset Class'''
class DatasetR(Dataset):
    """Youtube-VOS dataset."""

    def __init__(self, training_set, nb_users, transform=None):
        super(DatasetR, self).__init__()
        """
        Args:
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.training_set = training_set
        self.nb_users = nb_users
    def __len__(self):
        return self.nb_users

    def __getitem__(self, idx):
        #print(idx)
        sample = self.training_set[idx]


        #sample = torch.Tensor(sample)


        return sample

class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Batch Normalization.
        self.bn1 = nn.BatchNorm1d(20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Batch Normalization.
        self.bn2 = nn.BatchNorm1d(10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Batch Normalization
        self.bn3 = nn.BatchNorm1d(20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)
        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        # x = self.do1(self.bn1(self.activation(self.fc1(x))))
        # x = self.do2(self.bn2(self.activation(self.fc2(x))))
        # x = self.do3(self.bn3(self.activation(self.fc3(x))))

        x = self.bn1(self.activation(self.fc1(x)))
        x = self.bn2(self.activation(self.fc2(x)))
        x = self.bn3(self.activation(self.fc3(x)))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

          
    def predict(self, x): # x: visible nodes
        x = self.forward(x)
        return x

#Creating an instance of our SAE class
sae = SAE()

dataset = DatasetR(training_set = training_set, nb_users = nb_users)
train_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)


datasetTest = DatasetR(training_set = test_set, nb_users = nb_users)
test_loader = torch.utils.data.DataLoader(datasetTest, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)




 #Defining a criterion which specifies the metric to minimize. In this case, we want to minimize the MSE (Mean Squared Error)
criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. In this case, we'll use RMSprop
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)

# Setting the number of epochs

alosses = []

time_start = time.time()

nb_epoch = 200

# Iterating over each epoch

for epoch in range(1, nb_epoch + 1):

    #sae.train()

    # Initializing the train_loss which will be updated

    train_loss = 0

    # Initializing a counter

    s = 0.

    # Iterating over each user

    #for id_user in range(nb_users):

    for batch_idx, (sample) in enumerate(train_loader):

        # The input corresponds to the ratings given by the current user for each movie

        input = Variable(sample)

        target = input.clone()

        # We don't consider movies NOT rated by the current user. So we specify a conditional statement

        if torch.sum(target.data > 0) > 0:

            # We use our SAE to get the output from the 

            #print('input:  '+ str(input.shape))

            output = sae(input)

            #print(output.shape)

            target.require_grad = False

            output[target == 0] = 0

            # Defining our loss function, comparing the output with the target

            loss = criterion(output, target)

            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)

            # Computing the gradients necessary to adjust the weights

            loss.backward()

            # Updating the train_loss

            train_loss += np.sqrt(loss.data*mean_corrector)

            s += 1.

            # Updating the weights of the neural network

            optimizer.step()

    epoch_loss = train_loss / len(train_loader)
    alosses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, alosses, 'r', label='batch size = 500 ') # 'g' = color green
plt.title('Batch Normalization- epoch = 200')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')

plt.legend()
plt.show()

sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for input1,target1 in zip(train_loader,test_loader):
      input = Variable(input1)
      target = Variable(target1)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
  print('test_loss: '+str(test_loss/s))



from math import sqrt

RMSE = sqrt(0.0619)

RMSE

epochs = range(1,201)
plt.plot(epochs, losses, 'g', label='batch size = 200 ') # 'g' = color green
plt.plot(epochs, alosses, 'r', label='batch size = 500 ') # 'g' = color green
plt.title('Batch Normalization- epoch = 200')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')

plt.legend()
plt.show()



"""# Other Results

# Batch size 500 plus epoch size 500

*   TRAINING LOSS: 0.0474
*   TEST LOSS: 0.0512
*   RMSE LOSS:0.2262
"""

training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

training_set.shape

test_set.shape

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

batch_size  = 500

''' Dataset Class'''
class DatasetR(Dataset):
    """Youtube-VOS dataset."""

    def __init__(self, training_set, nb_users, transform=None):
        super(DatasetR, self).__init__()
        """
        Args:
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.training_set = training_set
        self.nb_users = nb_users
    def __len__(self):
        return self.nb_users

    def __getitem__(self, idx):
        #print(idx)
        sample = self.training_set[idx]


        #sample = torch.Tensor(sample)


        return sample

class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Batch Normalization.
        self.bn1 = nn.BatchNorm1d(20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Batch Normalization.
        self.bn2 = nn.BatchNorm1d(10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Batch Normalization
        self.bn3 = nn.BatchNorm1d(20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)
        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        # x = self.do1(self.bn1(self.activation(self.fc1(x))))
        # x = self.do2(self.bn2(self.activation(self.fc2(x))))
        # x = self.do3(self.bn3(self.activation(self.fc3(x))))

        x = self.bn1(self.activation(self.fc1(x)))
        x = self.bn2(self.activation(self.fc2(x)))
        x = self.bn3(self.activation(self.fc3(x)))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

          
    def predict(self, x): # x: visible nodes
        x = self.forward(x)
        return x

#Creating an instance of our SAE class
sae = SAE()

dataset = DatasetR(training_set = training_set, nb_users = nb_users)
train_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)



datasetTest = DatasetR(training_set = test_set, nb_users = nb_users)
test_loader = torch.utils.data.DataLoader(datasetTest, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)




 #Defining a criterion which specifies the metric to minimize. In this case, we want to minimize the MSE (Mean Squared Error)
criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. In this case, we'll use RMSprop
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)

# Setting the number of epochs

blosses = []

time_start = time.time()

nb_epoch = 500

# Iterating over each epoch

for epoch in range(1, nb_epoch + 1):

    #sae.train()

    # Initializing the train_loss which will be updated

    train_loss = 0

    # Initializing a counter

    s = 0.

    # Iterating over each user

    #for id_user in range(nb_users):

    for batch_idx, (sample) in enumerate(train_loader):

        # The input corresponds to the ratings given by the current user for each movie

        input = Variable(sample)

        target = input.clone()

        # We don't consider movies NOT rated by the current user. So we specify a conditional statement

        if torch.sum(target.data > 0) > 0:

            # We use our SAE to get the output from the 

            #print('input:  '+ str(input.shape))

            output = sae(input)

            #print(output.shape)

            target.require_grad = False

            output[target == 0] = 0

            # Defining our loss function, comparing the output with the target

            loss = criterion(output, target)

            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)

            # Computing the gradients necessary to adjust the weights

            loss.backward()

            # Updating the train_loss

            train_loss += np.sqrt(loss.data*mean_corrector)

            s += 1.

            # Updating the weights of the neural network

            optimizer.step()

    epoch_loss = train_loss / len(train_loader)
    blosses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,501)
plt.plot(epochs, blosses, 'b', label=' batch size = 500 ') # 'g' = color green
plt.title('Batch Normalization- epoch = 500')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')

plt.legend()
plt.show()

sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for input1,target1 in zip(train_loader,test_loader):
      input = Variable(input1)
      target = Variable(target1)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
  print('test_loss: '+str(test_loss/s))

from math import sqrt

RMSE = sqrt(0.0512)

RMSE



"""# Batch size 500 plus epoch size 1000

*   TRAINING LOSS: 0.0441
*   TEST LOSS: 0.0478
*   RMSE LOSS: 0.2186
"""

training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

training_set.shape

test_set.shape

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

batch_size  = 500

''' Dataset Class'''
class DatasetR(Dataset):
    """Youtube-VOS dataset."""

    def __init__(self, training_set, nb_users, transform=None):
        super(DatasetR, self).__init__()
        """
        Args:
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.training_set = training_set
        self.nb_users = nb_users
    def __len__(self):
        return self.nb_users

    def __getitem__(self, idx):
        #print(idx)
        sample = self.training_set[idx]


        #sample = torch.Tensor(sample)


        return sample

class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Batch Normalization.
        self.bn1 = nn.BatchNorm1d(20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Batch Normalization.
        self.bn2 = nn.BatchNorm1d(10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Batch Normalization
        self.bn3 = nn.BatchNorm1d(20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)
        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        # x = self.do1(self.bn1(self.activation(self.fc1(x))))
        # x = self.do2(self.bn2(self.activation(self.fc2(x))))
        # x = self.do3(self.bn3(self.activation(self.fc3(x))))

        x = self.bn1(self.activation(self.fc1(x)))
        x = self.bn2(self.activation(self.fc2(x)))
        x = self.bn3(self.activation(self.fc3(x)))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

          
    def predict(self, x): # x: visible nodes
        x = self.forward(x)
        return x

#Creating an instance of our SAE class
sae = SAE()

dataset = DatasetR(training_set = training_set, nb_users = nb_users)
train_loader = torch.utils.data.DataLoader(dataset, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)

# print('Train Len')
# print(training_set.shape)
# print(nb_users)
# print(test_set.shape)

datasetTest = DatasetR(training_set = test_set, nb_users = nb_users)
test_loader = torch.utils.data.DataLoader(datasetTest, batch_size = batch_size, shuffle=True, num_workers=4, drop_last=True)




 #Defining a criterion which specifies the metric to minimize. In this case, we want to minimize the MSE (Mean Squared Error)
criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. In this case, we'll use RMSprop
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)

# Setting the number of epochs

closses = []

time_start = time.time()

nb_epoch = 1000

# Iterating over each epoch

for epoch in range(1, nb_epoch + 1):

    #sae.train()

    # Initializing the train_loss which will be updated

    train_loss = 0

    # Initializing a counter

    s = 0.

    # Iterating over each user

    #for id_user in range(nb_users):

    for batch_idx, (sample) in enumerate(train_loader):

        # The input corresponds to the ratings given by the current user for each movie

        input = Variable(sample)

        target = input.clone()

        # We don't consider movies NOT rated by the current user. So we specify a conditional statement

        if torch.sum(target.data > 0) > 0:

            # We use our SAE to get the output from the 

            #print('input:  '+ str(input.shape))

            output = sae(input)

            #print(output.shape)

            target.require_grad = False

            output[target == 0] = 0

            # Defining our loss function, comparing the output with the target

            loss = criterion(output, target)

            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)

            # Computing the gradients necessary to adjust the weights

            loss.backward()

            # Updating the train_loss

            train_loss += np.sqrt(loss.data*mean_corrector)

            s += 1.

            # Updating the weights of the neural network

            optimizer.step()

    epoch_loss = train_loss / len(train_loader)
    closses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,1001)
plt.plot(epochs, closses, 'c', label=' batch size = 500 ') # 'g' = color green
plt.title('Batch Normalization- epoch = 1000')
plt.xlabel('Epochs')
plt.ylabel('Training Loss')

plt.legend()
plt.show()

sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for input1,target1 in zip(train_loader,test_loader):
      input = Variable(input1)
      target = Variable(target1)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
  print('test_loss: '+str(test_loss/s))

from math import sqrt

RMSE = sqrt(0.0478)

RMSE



