# -*- coding: utf-8 -*-
"""ResearchProject(Optimizer Comparison).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dasf-xQszgL0jdLtklrySZNtIiboqF1n
"""



"""# **Building A Movie Recommandation System With Pytorch(Optimizers)**
In this project, we are trying to predict the ratings that a user will give to an unseen movie, based on the ratings he gave to other movies. We will use the movielens dataset.The Main folder, which is ml-100k contains informations about 100 000 movies. We will use AutoEncoders to create our recommandation system. Let's start by importing the required libraries.
"""



# AutoEncoders

# Importing the libraries

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
from torchvision import datasets, transforms
from torch.utils.data import Dataset
import time

import matplotlib.pyplot as plt

from google.colab import files
uploaded = files.upload()

from google.colab import files
uploaded = files.upload()



"""# RMSprop

* TRAINING LOSS: 0.9147
* TEST LOSS: 0.9498
* RMSE: 0.9745
"""

training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Create the SAE (stack autoencoder) class, which is inherited from nn.Module(Pytorch library)
class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)

        #####  Activation Function: Sigmoid   #######

        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

    def predict(self, x): # x: visible nodes
        x = self.forward(x)
        return x

#Creating an instance of our SAE class
sae = SAE()


 #Defining a criterion which specifies the metric to minimize.
 #In this case, we want to minimize the MSE (Mean Squared Error)

criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. 
#In this case, we'll use RMSprop

optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)



#time
time_start = time.time()
# Setting the number of epochs
nb_epoch = 200

# storing the  traning loss calculations
losses = [] 

# Iterating over each epoch
for epoch in range(1, nb_epoch + 1):
    # Initializing the train_loss which will be updated
    train_loss = 0
    # Initializing a counter
    s = 0.
    # Iterating over each user
    for id_user in range(nb_users):
        # The input corresponds to the ratings given by the current user for each movie
        input = Variable(training_set[id_user]).unsqueeze(0)
        target = input.clone()
        # We don't consider movies NOT rated by the current user. So we specify a conditional statement
        if torch.sum(target.data > 0) > 0:
            # We use our SAE to get the output from the 
            output = sae(input)
            target.require_grad = False
            output[target == 0] = 0
            # Defining our loss function, comparing the output with the target
            loss = criterion(output, target)
            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
            # Computing the gradients necessary to adjust the weights
            loss.backward()
            # Updating the train_loss
            train_loss += np.sqrt(loss.data*mean_corrector)
            s += 1.
            # Updating the weights of the neural network
            optimizer.step()

    epoch_loss = train_loss / len(training_set[id_user])
    losses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, losses, 'g', label=' RMSprop ') # 'g' = color green
plt.title('Optimzer Loss - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()

# Initializing the test_loss
sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for id_user in range(nb_users):
      input = Variable(training_set[id_user]).unsqueeze(0)
      target = Variable(test_set[id_user]).unsqueeze(0)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
  
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
  print('test_loss: '+str(test_loss/s))

from math import sqrt

RMSE = sqrt(0.9498)

RMSE

"""#  Optimizer: ADAM

* TRAINING LOSS: 0.8985
* TEST LOSS: 0.9395
* RMSE: 0.9692
"""



# AutoEncoders

# Importing the libraries

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
from torch.autograd import Variable
from torchvision import datasets, transforms
from torch.utils.data import Dataset
import time

import matplotlib.pyplot as plt

training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Create the SAE (stack autoencoder) class, which is inherited from nn.Module(Pytorch library)
class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)

        #####  Activation Function: Sigmoid   #######

        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

#Creating an instance of our SAE class
sae = SAE()


 #Defining a criterion which specifies the metric to minimize.
 #In this case, we want to minimize the MSE (Mean Squared Error)

criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. 
#In this case, we'll use Adam

optimizer = optim.Adam(sae.parameters(), lr = 0.01, weight_decay = 0.5)

#time
time_start = time.time()
# Setting the number of epochs
nb_epoch = 200

# storing the  traning loss calculations
alosses = [] 

# Iterating over each epoch
for epoch in range(1, nb_epoch + 1):
    # Initializing the train_loss which will be updated
    train_loss = 0
    # Initializing a counter
    s = 0.
    # Iterating over each user
    for id_user in range(nb_users):
        # The input corresponds to the ratings given by the current user for each movie
        input = Variable(training_set[id_user]).unsqueeze(0)
        target = input.clone()
        # We don't consider movies NOT rated by the current user. So we specify a conditional statement
        if torch.sum(target.data > 0) > 0:
            # We use our SAE to get the output from the 
            output = sae(input)
            target.require_grad = False
            output[target == 0] = 0
            # Defining our loss function, comparing the output with the target
            loss = criterion(output, target)
            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
            # Computing the gradients necessary to adjust the weights
            loss.backward()
            # Updating the train_loss
            train_loss += np.sqrt(loss.data*mean_corrector)
            s += 1.
            # Updating the weights of the neural network
            optimizer.step()

    epoch_loss = train_loss / len(training_set[id_user])
    alosses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, alosses, 'b', label=' ADAM ') # 'g' = color green
plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()

# Initializing the test_loss
sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for id_user in range(nb_users):
      input = Variable(training_set[id_user]).unsqueeze(0)
      target = Variable(test_set[id_user]).unsqueeze(0)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
  
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
  print('test_loss: '+str(test_loss/s))

from math import sqrt

RMSE = sqrt(0.9395)

RMSE

epochs = range(1,201)
plt.plot(epochs, slosses, 'g', label=' RMSprop ')
plt.plot(epochs, alosses, 'b', label='ADAM') # 'r' = color green

plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()



"""# Optimizer: SGD

* TRAINING LOSS: 1.0119
* TEST LOSS: 1.0189
* RMSE: 1.0094
"""



training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Create the SAE (stack autoencoder) class, which is inherited from nn.Module(Pytorch library)
class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)

        #####  Activation Function: Sigmoid   #######

        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

#Creating an instance of our SAE class
sae = SAE()


 #Defining a criterion which specifies the metric to minimize.
 #In this case, we want to minimize the MSE (Mean Squared Error)

criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. 
#In this case, we'll use SGD

optimizer = optim.SGD(sae.parameters(), lr = 0.01, weight_decay = 0.5)

#time
time_start = time.time()
# Setting the number of epochs
nb_epoch = 200

# storing the  traning loss calculations
slosses = [] 

# Iterating over each epoch
for epoch in range(1, nb_epoch + 1):
    # Initializing the train_loss which will be updated
    train_loss = 0
    # Initializing a counter
    s = 0.
    # Iterating over each user
    for id_user in range(nb_users):
        # The input corresponds to the ratings given by the current user for each movie
        input = Variable(training_set[id_user]).unsqueeze(0)
        target = input.clone()
        # We don't consider movies NOT rated by the current user. So we specify a conditional statement
        if torch.sum(target.data > 0) > 0:
            # We use our SAE to get the output from the 
            output = sae(input)
            target.require_grad = False
            output[target == 0] = 0
            # Defining our loss function, comparing the output with the target
            loss = criterion(output, target)
            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
            # Computing the gradients necessary to adjust the weights
            loss.backward()
            # Updating the train_loss
            train_loss += np.sqrt(loss.data*mean_corrector)
            s += 1.
            # Updating the weights of the neural network
            optimizer.step()

    epoch_loss = train_loss / len(training_set[id_user])
    slosses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, slosses, 'r', label=' SGD ') # 'g' = color green
plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()

# Initializing the test_loss
sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for id_user in range(nb_users):
      input = Variable(training_set[id_user]).unsqueeze(0)
      target = Variable(test_set[id_user]).unsqueeze(0)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
  
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
          pred = sae(target)
  print('test_loss: '+str(test_loss/s))

from math import sqrt

RMSE = sqrt(1.0189)

RMSE

epochs = range(1,201)
plt.plot(epochs, losses, 'g', label='RMSprop')
plt.plot(epochs, alosses, 'b', label='ADAM')
plt.plot(epochs, slosses, 'r', label=' SGD ')


plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()

"""#  Optimizer: ADAGRAD

* TRAINING LOSS: 0.9117
* TEST LOSS: 0.9515
* RMSE: 0.9754
"""



training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Create the SAE (stack autoencoder) class, which is inherited from nn.Module(Pytorch library)
class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)

        #####  Activation Function: Sigmoid   #######

        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

#Creating an instance of our SAE class
sae = SAE()


 #Defining a criterion which specifies the metric to minimize.
 #In this case, we want to minimize the MSE (Mean Squared Error)

criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. 
#In this case, we'll use Adagrad

optimizer = optim.Adagrad(sae.parameters(), lr = 0.01, weight_decay = 0.5)

#time
time_start = time.time()
# Setting the number of epochs
nb_epoch = 200

# storing the  traning loss calculations
adlosses = [] 

# Iterating over each epoch
for epoch in range(1, nb_epoch + 1):
    # Initializing the train_loss which will be updated
    train_loss = 0
    # Initializing a counter
    s = 0.
    # Iterating over each user
    for id_user in range(nb_users):
        # The input corresponds to the ratings given by the current user for each movie
        input = Variable(training_set[id_user]).unsqueeze(0)
        target = input.clone()
        # We don't consider movies NOT rated by the current user. So we specify a conditional statement
        if torch.sum(target.data > 0) > 0:
            # We use our SAE to get the output from the 
            output = sae(input)
            target.require_grad = False
            output[target == 0] = 0
            # Defining our loss function, comparing the output with the target
            loss = criterion(output, target)
            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
            # Computing the gradients necessary to adjust the weights
            loss.backward()
            # Updating the train_loss
            train_loss += np.sqrt(loss.data*mean_corrector)
            s += 1.
            # Updating the weights of the neural network
            optimizer.step()

    epoch_loss = train_loss / len(training_set[id_user])
    adlosses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, adlosses, 'y', label=' Adagrad') # 'g' = color green
plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()

# Initializing the test_loss
sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for id_user in range(nb_users):
      input = Variable(training_set[id_user]).unsqueeze(0)
      target = Variable(test_set[id_user]).unsqueeze(0)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
  
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
          pred = sae(target)
  print('test_loss: '+str(test_loss/s))

from math import sqrt

RMSE = sqrt(0.9515)

RMSE

epochs = range(1,201)
plt.plot(epochs, losses, 'g', label='RMSprop')
plt.plot(epochs, alosses, 'b', label='ADAM')
plt.plot(epochs, slosses, 'r', label=' SGD ')
plt.plot(epochs, adlosses, 'y', label=' Adagrad ')



plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()



"""# Optimizer: AdaDelta

* TRAINING LOSS: 1.0142
* TEST LOSS: 1.0216
* RMSE: 1.0107
"""



training = ['user_id', 'movie_id', 'rating', 'timestamp' ] #Create each column
test = ['user_id', 'movie_id', 'rating', 'timestamp' ]     #Create each column

# Preparing the training set and the test set 
training_set = pd.read_csv('u1.base', names=training, delimiter = '\t') # Read the file
test_set = pd.read_csv('u1.test', names=test, delimiter = '\t') #Read the file

#Drop 'timestamp' column
training_set= training_set.drop(["timestamp"], axis=1)

#Drop 'timestamp' column
test_set= test_set.drop(["timestamp"], axis=1)

# Visualizing the first elements of the training_set
training_set.head()

# Converting the training and test sets into numpy arrays
training_set = np.array(training_set, dtype = 'int')
test_set = np.array(test_set, dtype = 'int')

# Getting the number of users and movies
nb_users = int(max(max(training_set[:, 0]), max(test_set[:, 0])))
nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))

print("Number of users: {}".format(nb_users))
print("Number of movies: {}".format(nb_movies))

def convert(data):
    # Initializing an empty list that will take the list of ratings given by a specific user
    new_data = []
    # Looping over all the users
    for id_users in range(1, nb_users + 1):
        # We get the id of the movies rated by the current user
        id_movies = data[:, 1][data[:, 0] == id_users]
        # We get the id of the ratings given by the current_user
        id_ratings = data[:, 2][data[:, 0] == id_users]
        # 
        ratings = np.zeros(nb_movies)
        # For movies rated by the current user, we replace 0 with the rating
        # The first element of ratings is at index 0. However, id_movies start at index 1.
        # Therefore, ratings[id_movies - 1] will correspond to the location of the movie we're considering
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data

# Applying the convert function to the training and test set.
training_set = convert(training_set)
test_set = convert(test_set)

# Convert the data into Torch tensors
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)

# Create the SAE (stack autoencoder) class, which is inherited from nn.Module(Pytorch library)
class SAE(nn.Module):
    # Initializing the class
    def __init__(self, ):
        # making the class get all the functions from the parent class nn.Module
        super(SAE, self).__init__()
        # Creating the first encoding layer. The number of input corresponds to the number of movies
        #  Decide to encode it into 20 outputs
        self.fc1 = nn.Linear(nb_movies, 20)
        # Creating the second encoding layer. From 20 inputs to 10 outputs
        self.fc2 = nn.Linear(20, 10)
        # Creating the first decoding layer. From 10 inputs to 20 outputs
        self.fc3 = nn.Linear(10, 20)
        # Creating the second hidden layer. From 20 inputs to nb_movies outputs
        self.fc4 = nn.Linear(20, nb_movies)

        #####  Activation Function: Sigmoid   #######

        # Creating the activation fucntion which will fire up specific neurons 
        self.activation = nn.Sigmoid()
        
        # Creating the function for forward propagation
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        # With autoencoder, we don't need an activation function for the last decoding part
        x = self.fc4(x)
        return x

#Creating an instance of our SAE class
sae = SAE()


 #Defining a criterion which specifies the metric to minimize.
 #In this case, we want to minimize the MSE (Mean Squared Error)

criterion = nn.MSELoss()

# Defining the algorithm used to minimize the loss function. 
#In this case, we'll use Adadelta

optimizer = optim.Adadelta(sae.parameters(), lr = 0.01, weight_decay = 0.5)

#time
time_start = time.time()
# Setting the number of epochs
nb_epoch = 200

# storing the  traning loss calculations
addlosses = [] 

# Iterating over each epoch
for epoch in range(1, nb_epoch + 1):
    # Initializing the train_loss which will be updated
    train_loss = 0
    # Initializing a counter
    s = 0.
    # Iterating over each user
    for id_user in range(nb_users):
        # The input corresponds to the ratings given by the current user for each movie
        input = Variable(training_set[id_user]).unsqueeze(0)
        target = input.clone()
        # We don't consider movies NOT rated by the current user. So we specify a conditional statement
        if torch.sum(target.data > 0) > 0:
            # We use our SAE to get the output from the 
            output = sae(input)
            target.require_grad = False
            output[target == 0] = 0
            # Defining our loss function, comparing the output with the target
            loss = criterion(output, target)
            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
            # Computing the gradients necessary to adjust the weights
            loss.backward()
            # Updating the train_loss
            train_loss += np.sqrt(loss.data*mean_corrector)
            s += 1.
            # Updating the weights of the neural network
            optimizer.step()

    epoch_loss = train_loss / len(training_set[id_user])
    addlosses.append(epoch_loss)  
    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))
time_end = time.time()
print('Stacked-Autoencoder(SAE) Training Time : ' +str(round((time_end-time_start)/60,0))+' Minutes. ')

epochs = range(1,201)
plt.plot(epochs, addlosses, 'c', label=' Adadelta ') # 'g' = color green
plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()

# Initializing the test_loss
sae.eval()
with torch.no_grad():
  test_loss = 0
  s = 0.
  for id_user in range(nb_users):
      input = Variable(training_set[id_user]).unsqueeze(0)
      target = Variable(test_set[id_user]).unsqueeze(0)
      if torch.sum(target.data > 0) > 0:
          output = sae(input)
          target.require_grad = False
          output[(target == 0)] = 0
          loss = criterion(output, target)
  
          mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)
          test_loss += np.sqrt(loss.data*mean_corrector)
          s += 1.
          pred = sae(target)
  print('test_loss: '+str(test_loss/s))



from math import sqrt

RMSE = sqrt(1.0216)

RMSE

epochs = range(1,201)
plt.plot(epochs, losses, 'g', label='RMSprop')
plt.plot(epochs, alosses, 'b', label='ADAM')
plt.plot(epochs, slosses, 'r', label=' SGD ')
plt.plot(epochs, adlosses, 'y', label=' Adagrad ')
plt.plot(epochs, addlosses, 'c', label=' Adadelta ')


plt.title('Optimizer - epoch =200')
plt.xlabel('Epochs')
plt.ylabel('Training  Loss')

plt.legend()
plt.show()